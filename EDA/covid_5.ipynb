{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ-HgyUobG-S",
        "colab_type": "code",
        "outputId": "f27bde81-25f2-49ea-f190-65264c55b71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install  kaggle\n",
        "\n",
        "!mkdir .kaggle\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "mkdir: cannot create directory ‚Äò.kaggle‚Äô: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAi3U_JNby1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"abolimarathe\",\"key\":\"53f6206052b7e46e5736781e4a76ca15\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLFjCySxb1nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4r_KeO9b5CM",
        "colab_type": "code",
        "outputId": "7ebbc309-1cf9-4165-b5a3-3ff7572cb369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!kaggle config set -n path -v{/content}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xP28miY9b9GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW-kud_Ibffc",
        "colab_type": "code",
        "outputId": "a6604c65-7bca-4e32-a7e8-738ba4b4d7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                           title                                                size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "allen-institute-for-ai/CORD-19-research-challenge             COVID-19 Open Research Dataset Challenge (CORD-19)    3GB  2020-06-05 02:43:46          71785  \n",
            "roche-data-science-coalition/uncover                          UNCOVER COVID-19 Challenge                          179MB  2020-05-21 18:57:53          10483  \n",
            "siddharthm1698/coursera-course-dataset                        Coursera Course Dataset                              23KB  2020-05-25 05:52:27             10  \n",
            "stefanlarson/outofscope-intent-classification-dataset         Out-of-Scope Intent Classification Dataset          285KB  2020-05-15 03:04:34              1  \n",
            "kianwee/agricultural-raw-material-prices-19902020             Agricultural Raw Material prices (1990-2020)         23KB  2020-05-27 04:51:29              0  \n",
            "kushshah95/the-insurance-company-tic-benchmark                The Insurance Company (TIC) Benchmark               262KB  2020-05-27 16:36:41              1  \n",
            "biancaferreira/african-wildlife                               African Wildlife                                    448MB  2020-05-25 13:42:19              2  \n",
            "yamaerenay/spotify-dataset-19212020-160k-tracks               Spotify Dataset 1921-2020, 160k+ Tracks              17MB  2020-05-25 11:31:07             24  \n",
            "jessicali9530/animal-crossing-new-horizons-nookplaza-dataset  Animal Crossing New Horizons NookPlaza Catalog      577KB  2020-05-18 22:50:26             42  \n",
            "ruchi798/movies-on-netflix-prime-video-hulu-and-disney        Movies on Netflix, Prime Video, Hulu and Disney+    627KB  2020-05-22 23:48:01             47  \n",
            "ruchi798/malnutrition-across-the-globe                        Malnutrition across the globe                        79KB  2020-05-25 09:51:45             14  \n",
            "ruchi798/tv-shows-on-netflix-prime-video-hulu-and-disney      TV shows on Netflix, Prime Video, Hulu and Disney+   88KB  2020-05-25 15:38:39             12  \n",
            "rmjacobsen/property-listings-for-5-south-american-countries   Property Listings for 5 South American Countries    475MB  2020-05-25 02:05:38              1  \n",
            "jessemostipak/marble-racing                                   Marble Racing                                         4KB  2020-06-02 17:08:53              0  \n",
            "gomes555/road-transport-brazil                                Road transport dataset in brazil                     62MB  2020-05-26 02:52:34              4  \n",
            "devinaconley/covid19-mobility-data                            COVID-19 Google mobility data                         4MB  2020-06-04 12:15:15              2  \n",
            "benroshan/factors-affecting-campus-placement                  Campus Recruitment                                    5KB  2020-04-11 11:09:02           5523  \n",
            "bobbyscience/league-of-legends-diamond-ranked-games-10-min    League of Legends Diamond Ranked Games (10 min)     539KB  2020-04-13 13:53:02           2542  \n",
            "fireballbyedimyrnmom/us-counties-covid-19-dataset             US counties COVID 19 dataset                          2MB  2020-06-05 11:59:45           7247  \n",
            "divyansh22/flight-delay-prediction                            January Flight Delay Prediction                      23MB  2020-04-14 13:15:41           1850  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwB4g-1sbpph",
        "colab_type": "code",
        "outputId": "aa3fc6d0-3dbe-4af7-a083-6acf79922f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d smid80/coronavirus-covid19-tweets-early-april -p /content"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coronavirus-covid19-tweets-early-april.zip to /content\n",
            "100% 1.30G/1.30G [00:30<00:00, 73.1MB/s]\n",
            "100% 1.30G/1.30G [00:30<00:00, 46.1MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04kqK3oncEzF",
        "colab_type": "code",
        "outputId": "8a5bcf58-a176-413d-9c3e-02456bda6da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!unzip \\*.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  coronavirus-covid19-tweets-early-april.zip\n",
            "  inflating: 2020-03-29 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-03-30 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-03-31 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-01 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-02 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-03 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-04 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-05 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-06 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-07 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-08 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-09 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-10 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-11 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-12 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-13 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-14 Coronavirus Tweets.CSV  \n",
            "  inflating: 2020-04-15 Coronavirus Tweets.CSV  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFQdqAPhcJKk",
        "colab_type": "code",
        "outputId": "9e13e1d1-43e3-40bf-edf4-0127ecccfa74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from gensim import models, corpora \n",
        "import spacy\n",
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.tokenizer import _get_regex_pattern\n",
        "nlp = spacy.load(\"en_core_web_sm\") \n",
        "import numpy as np \n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_11Eizgq-vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = pd.read_csv(\"2020-03-29 Coronavirus Tweets.CSV\")\n",
        "d2 = pd.read_csv(\"2020-03-30 Coronavirus Tweets.CSV\")\n",
        "d3 = pd.read_csv(\"2020-03-31 Coronavirus Tweets.CSV\")\n",
        "d4 = pd.read_csv(\"2020-04-01 Coronavirus Tweets.CSV\")\n",
        "d5 = pd.read_csv(\"2020-04-02 Coronavirus Tweets.CSV\")\n",
        "d6 = pd.read_csv(\"2020-04-03 Coronavirus Tweets.CSV\")\n",
        "d7 = pd.read_csv(\"2020-04-04 Coronavirus Tweets.CSV\")\n",
        "d8 = pd.read_csv(\"2020-04-05 Coronavirus Tweets.CSV\")\n",
        "d9 = pd.read_csv(\"2020-04-06 Coronavirus Tweets.CSV\")\n",
        "d10 = pd.read_csv(\"2020-04-07 Coronavirus Tweets.CSV\")\n",
        "d11 = pd.read_csv(\"2020-04-08 Coronavirus Tweets.CSV\")\n",
        "d12 = pd.read_csv(\"2020-04-09 Coronavirus Tweets.CSV\")\n",
        "d13 = pd.read_csv(\"2020-04-10 Coronavirus Tweets.CSV\")\n",
        "d14 = pd.read_csv(\"2020-04-11 Coronavirus Tweets.CSV\")\n",
        "d15 = pd.read_csv(\"2020-04-12 Coronavirus Tweets.CSV\")\n",
        "d16 = pd.read_csv(\"2020-04-13 Coronavirus Tweets.CSV\")\n",
        "d17 = pd.read_csv(\"2020-04-14 Coronavirus Tweets.CSV\")\n",
        "d18 = pd.read_csv(\"2020-04-15 Coronavirus Tweets.CSV\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av-HkiLjcJHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1= d1[['text','lang','country_code','created_at']]\n",
        "d1 = d1[d1['lang']=='en']\n",
        "\n",
        "d2= d2[['text','lang','country_code','created_at']]\n",
        "d2 = d2[d2['lang']=='en']\n",
        "\n",
        "d3= d3[['text','lang','country_code','created_at']]\n",
        "d3 = d3[d3['lang']=='en']\n",
        "\n",
        "d4= d4[['text','lang','country_code','created_at']]\n",
        "d4 = d4[d4['lang']=='en']\n",
        "\n",
        "d5= d5[['text','lang','country_code','created_at']]\n",
        "d5 = d5[d5['lang']=='en']\n",
        "\n",
        "d6= d6[['text','lang','country_code','created_at']]\n",
        "d6 = d6[d6['lang']=='en']\n",
        "\n",
        "d7= d7[['text','lang','country_code','created_at']]\n",
        "d7 = d7[d7['lang']=='en']\n",
        "\n",
        "d8= d8[['text','lang','country_code','created_at']]\n",
        "d8 = d8[d8['lang']=='en']\n",
        "\n",
        "d9= d9[['text','lang','country_code','created_at']]\n",
        "d9 = d9[d9['lang']=='en']\n",
        "\n",
        "d10= d10[['text','lang','country_code','created_at']]\n",
        "d10 = d10[d10['lang']=='en']\n",
        "\n",
        "d11= d11[['text','lang','country_code','created_at']]\n",
        "d11 = d11[d11['lang']=='en']\n",
        "\n",
        "d12= d12[['text','lang','country_code','created_at']]\n",
        "d12 = d12[d12['lang']=='en']\n",
        "\n",
        "d13= d13[['text','lang','country_code','created_at']]\n",
        "d13 = d13[d13['lang']=='en']\n",
        "\n",
        "d14= d14[['text','lang','country_code','created_at']]\n",
        "d14 = d14[d14['lang']=='en']\n",
        "\n",
        "d15= d15[['text','lang','country_code','created_at']]\n",
        "d15 = d15[d15['lang']=='en']\n",
        "\n",
        "d16= d16[['text','lang','country_code','created_at']]\n",
        "d16 = d16[d16['lang']=='en']\n",
        "\n",
        "d17= d17[['text','lang','country_code','created_at']]\n",
        "d17 = d17[d17['lang']=='en']\n",
        "\n",
        "d18= d18[['text','lang','country_code','created_at']]\n",
        "d18 = d18[d18['lang']=='en']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "locjKPPVcJDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1.dropna(inplace=True)\n",
        "d2.dropna(inplace=True)\n",
        "d3.dropna(inplace=True)\n",
        "d4.dropna(inplace=True)\n",
        "d5.dropna(inplace=True)\n",
        "d6.dropna(inplace=True)\n",
        "d7.dropna(inplace=True)\n",
        "d8.dropna(inplace=True)\n",
        "d9.dropna(inplace=True)\n",
        "d10.dropna(inplace=True)\n",
        "d11.dropna(inplace=True)\n",
        "d12.dropna(inplace=True)\n",
        "d13.dropna(inplace=True)\n",
        "d14.dropna(inplace=True)\n",
        "d15.dropna(inplace=True)\n",
        "d16.dropna(inplace=True)\n",
        "d17.dropna(inplace=True)\n",
        "d18.dropna(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpX7LRJJcJAo",
        "colab_type": "code",
        "outputId": "a114dcdd-73df-4c1a-f215-f25eb1165792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d3"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>This is how far back they have to put the swab...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T00:00:04Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>Mayor @KeishaBottoms to WH on #COVID19 - ‚ÄúAllo...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T00:00:05Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>Big trip 5 mi to north...checked out Yountvill...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T00:00:18Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>Nurses are being threatened and disciplined fo...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T00:00:21Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>What colour is that virus? #coronavirus https:...</td>\n",
              "      <td>en</td>\n",
              "      <td>AU</td>\n",
              "      <td>2020-03-31T00:00:25Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669727</th>\n",
              "      <td>YOU OK WITH THIS DOCTOR SENATOR @BillCassidy ?...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T23:59:40Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669736</th>\n",
              "      <td>This a good look? üò∑\\n‚ñ™Ô∏è‚ñ™Ô∏è‚ñ™Ô∏è\\n‚ñ™Ô∏è‚ñ™Ô∏è‚ñ™Ô∏è\\n#coronavi...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T23:59:42Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669753</th>\n",
              "      <td>Every question from each journalist at tomorro...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-31T23:59:46Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669789</th>\n",
              "      <td>This may be the first time I actually want som...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-31T23:59:53Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669795</th>\n",
              "      <td>It‚Äôs my Birthday üéÇ #stuck #inside #coronavirus...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-31T23:59:54Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18381 rows √ó 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  ...            created_at\n",
              "203     This is how far back they have to put the swab...  ...  2020-03-31T00:00:04Z\n",
              "220     Mayor @KeishaBottoms to WH on #COVID19 - ‚ÄúAllo...  ...  2020-03-31T00:00:05Z\n",
              "394     Big trip 5 mi to north...checked out Yountvill...  ...  2020-03-31T00:00:18Z\n",
              "435     Nurses are being threatened and disciplined fo...  ...  2020-03-31T00:00:21Z\n",
              "485     What colour is that virus? #coronavirus https:...  ...  2020-03-31T00:00:25Z\n",
              "...                                                   ...  ...                   ...\n",
              "669727  YOU OK WITH THIS DOCTOR SENATOR @BillCassidy ?...  ...  2020-03-31T23:59:40Z\n",
              "669736  This a good look? üò∑\\n‚ñ™Ô∏è‚ñ™Ô∏è‚ñ™Ô∏è\\n‚ñ™Ô∏è‚ñ™Ô∏è‚ñ™Ô∏è\\n#coronavi...  ...  2020-03-31T23:59:42Z\n",
              "669753  Every question from each journalist at tomorro...  ...  2020-03-31T23:59:46Z\n",
              "669789  This may be the first time I actually want som...  ...  2020-03-31T23:59:53Z\n",
              "669795  It‚Äôs my Birthday üéÇ #stuck #inside #coronavirus...  ...  2020-03-31T23:59:54Z\n",
              "\n",
              "[18381 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyZ7EZ-vcI9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1.reset_index(inplace = True) \n",
        "d2.reset_index(inplace = True)\n",
        "d3.reset_index(inplace = True)\n",
        "d4.reset_index(inplace = True)\n",
        "d5.reset_index(inplace = True)\n",
        "d6.reset_index(inplace = True)\n",
        "d7.reset_index(inplace = True)\n",
        "d8.reset_index(inplace = True)\n",
        "d9.reset_index(inplace = True)\n",
        "d10.reset_index(inplace = True)\n",
        "d11.reset_index(inplace = True)\n",
        "d12.reset_index(inplace = True)\n",
        "d13.reset_index(inplace = True)\n",
        "d14.reset_index(inplace = True)\n",
        "d15.reset_index(inplace = True)\n",
        "d16.reset_index(inplace = True)\n",
        "d17.reset_index(inplace = True)\n",
        "d18.reset_index(inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "new = d1[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d1[\"date\"]= new[0] \n",
        "d1[\"time\"]= new[1] \n",
        "\n",
        "new = d2[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d2[\"date\"]= new[0] \n",
        "d2[\"time\"]= new[1] \n",
        "\n",
        "new = d3[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d3[\"date\"]= new[0] \n",
        "d3[\"time\"]= new[1] \n",
        "\n",
        "new = d4[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d4[\"date\"]= new[0] \n",
        "d4[\"time\"]= new[1] \n",
        "\n",
        "new = d5[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d5[\"date\"]= new[0] \n",
        "d5[\"time\"]= new[1] \n",
        "\n",
        "new = d6[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d6[\"date\"]= new[0] \n",
        "d6[\"time\"]= new[1] \n",
        "\n",
        "new = d7[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d7[\"date\"]= new[0] \n",
        "d7[\"time\"]= new[1] \n",
        "\n",
        "new = d8[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d8[\"date\"]= new[0] \n",
        "d8[\"time\"]= new[1] \n",
        "\n",
        "new = d9[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d9[\"date\"]= new[0] \n",
        "d9[\"time\"]= new[1] \n",
        "\n",
        "new = d10[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d10[\"date\"]= new[0] \n",
        "d10[\"time\"]= new[1] \n",
        "\n",
        "new = d11[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d11[\"date\"]= new[0] \n",
        "d11[\"time\"]= new[1] \n",
        "\n",
        "new = d12[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d12[\"date\"]= new[0] \n",
        "d12[\"time\"]= new[1] \n",
        "\n",
        "new = d13[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d13[\"date\"]= new[0] \n",
        "d13[\"time\"]= new[1] \n",
        "\n",
        "new = d14[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d14[\"date\"]= new[0] \n",
        "d14[\"time\"]= new[1] \n",
        "\n",
        "new = d15[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d15[\"date\"]= new[0] \n",
        "d15[\"time\"]= new[1] \n",
        "\n",
        "new = d16[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d16[\"date\"]= new[0] \n",
        "d16[\"time\"]= new[1] \n",
        "\n",
        "new = d17[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d17[\"date\"]= new[0] \n",
        "d17[\"time\"]= new[1] \n",
        "\n",
        "new = d18[\"created_at\"].str.split(\"T\", n = 1, expand = True)\n",
        "d18[\"date\"]= new[0] \n",
        "d18[\"time\"]= new[1] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bkk-KtkcI52",
        "colab_type": "code",
        "outputId": "6ffd0c10-39b2-445e-a858-4ce0658e5075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(d1) +len(d2)+len(d3)+len(d4)+len(d5)+len(d6)+len(d7)+len(d8)+len(d9)+len(d10)+len(d11)+len(d12)+len(d13)+len(d14)+len(d15)+len(d16)+len(d17)+len(d18)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230769"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCuvuwHAcI2e",
        "colab_type": "code",
        "outputId": "3f312925-4f2e-48e4-e0e6-2d82f14c3b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UzH-FD3tmTX",
        "colab_type": "code",
        "outputId": "4c1c7dc2-c8c7-4b80-8b79-4a4f4eb27fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>293</td>\n",
              "      <td>@ClayTravis FLU? This seems fishy to me these ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:16Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:16Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>393</td>\n",
              "      <td>@netflix omg!!! Ozark ending was to die for!!!...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:29Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:29Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>481</td>\n",
              "      <td>I don't think there is gd leadership across th...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:39Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:39Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>499</td>\n",
              "      <td>In a change of conversation from #covid19 here...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500</td>\n",
              "      <td>Any other hospital doing this? Our engineering...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17850</th>\n",
              "      <td>564053</td>\n",
              "      <td>I am surprised too &amp;gt; ‚ÄúLocal leaders and som...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T23:59:41Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>23:59:41Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17851</th>\n",
              "      <td>564102</td>\n",
              "      <td>In my chair, tv on awaiting the 1pm briefing w...</td>\n",
              "      <td>en</td>\n",
              "      <td>NZ</td>\n",
              "      <td>2020-03-29T23:59:52Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>23:59:52Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17852</th>\n",
              "      <td>564113</td>\n",
              "      <td>Another 30! #socialdistancing #coronavirus #co...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T23:59:54Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>23:59:54Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17853</th>\n",
              "      <td>564114</td>\n",
              "      <td>#Covid_19 relief. Better things to talk about ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T23:59:54Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>23:59:54Z</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17854</th>\n",
              "      <td>564119</td>\n",
              "      <td>@nichmelbourne @GemmaTognini @ScottMorrisonMP ...</td>\n",
              "      <td>en</td>\n",
              "      <td>AU</td>\n",
              "      <td>2020-03-29T23:59:55Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>23:59:55Z</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17855 rows √ó 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        index  ...       time\n",
              "0         293  ...  00:00:16Z\n",
              "1         393  ...  00:00:29Z\n",
              "2         481  ...  00:00:39Z\n",
              "3         499  ...  00:00:42Z\n",
              "4         500  ...  00:00:42Z\n",
              "...       ...  ...        ...\n",
              "17850  564053  ...  23:59:41Z\n",
              "17851  564102  ...  23:59:52Z\n",
              "17852  564113  ...  23:59:54Z\n",
              "17853  564114  ...  23:59:54Z\n",
              "17854  564119  ...  23:59:55Z\n",
              "\n",
              "[17855 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9zFfBGmNY9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TZih0WszXWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = d1.append(d2)\n",
        "d1 =d1.append(d3)\n",
        "d1 =d1.append(d4)\n",
        "d1 =d1.append(d5)\n",
        "d1 =d1.append(d6)\n",
        "d1 =d1.append(d7)\n",
        "d1 =d1.append(d8)\n",
        "d1 =d1.append(d9)\n",
        "d1 =d1.append(d10)\n",
        "d1 =d1.append(d11)\n",
        "d1 =d1.append(d12)\n",
        "d1 =d1.append(d13)\n",
        "d1 =d1.append(d14)\n",
        "d1 =d1.append(d15)\n",
        "d1 =d1.append(d16)\n",
        "d1 =d1.append(d17)\n",
        "d1 =d1.append(d18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuQ1uNV4NbCa",
        "colab_type": "code",
        "outputId": "d4f13ec9-3348-409c-d991-f08ad6537053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "hashtags = d1['text'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*',x))\n",
        "print(hashtags)\n",
        "print(len(hashtags))\n",
        "d1['hashtag'] = hashtags"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0                                              [#Covid_19]\n",
            "1                                               [#COVID19]\n",
            "2                                               [#COVID19]\n",
            "3        [#covid19, #cavalierkingcharles, #daftdug, #La...\n",
            "4                                 [#Covid_19, #medtwitter]\n",
            "                               ...                        \n",
            "11415        [#WHO, #COVID19, #coronavirus, #GlobalHealth]\n",
            "11416             [#COVID19, #everyproblemisanopportunity]\n",
            "11417                             [#coronavirus, #Science]\n",
            "11418                                           [#COVID19]\n",
            "11419                               [#CoronavirusPandemic]\n",
            "Name: text, Length: 230769, dtype: object\n",
            "230769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ld1mw5zzO9",
        "colab_type": "code",
        "outputId": "3dd30474-fafa-452d-9e14-52e6f75c1fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(d1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230769"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kenieRUBzzLs",
        "colab_type": "code",
        "outputId": "8cd61cca-2209-4d54-9e62-4d842c8ff7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>293</td>\n",
              "      <td>@ClayTravis FLU? This seems fishy to me these ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:16Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:16Z</td>\n",
              "      <td>[#Covid_19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>393</td>\n",
              "      <td>@netflix omg!!! Ozark ending was to die for!!!...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:29Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:29Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>481</td>\n",
              "      <td>I don't think there is gd leadership across th...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:39Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:39Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>499</td>\n",
              "      <td>In a change of conversation from #covid19 here...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#covid19, #cavalierkingcharles, #daftdug, #La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>500</td>\n",
              "      <td>Any other hospital doing this? Our engineering...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#Covid_19, #medtwitter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11415</th>\n",
              "      <td>475039</td>\n",
              "      <td>‚ÄúThe #WHO is bureaucratic, frustrating, timid ...</td>\n",
              "      <td>en</td>\n",
              "      <td>CA</td>\n",
              "      <td>2020-04-15T23:59:07Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:07Z</td>\n",
              "      <td>[#WHO, #COVID19, #coronavirus, #GlobalHealth]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11416</th>\n",
              "      <td>475072</td>\n",
              "      <td>@CEO_CleMetParks sir! Some positive offshoots ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#COVID19, #everyproblemisanopportunity]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11417</th>\n",
              "      <td>475075</td>\n",
              "      <td>The good news is that the Iranian Revolutionar...</td>\n",
              "      <td>en</td>\n",
              "      <td>IR</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#coronavirus, #Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11418</th>\n",
              "      <td>475191</td>\n",
              "      <td>Remember when the Soviet Union told the world ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:46Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:46Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11419</th>\n",
              "      <td>475199</td>\n",
              "      <td>@UMNExtFD @UMNExt @FOX9 And here's the story w...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:47Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:47Z</td>\n",
              "      <td>[#CoronavirusPandemic]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230769 rows √ó 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        index  ...                                            hashtag\n",
              "0         293  ...                                        [#Covid_19]\n",
              "1         393  ...                                         [#COVID19]\n",
              "2         481  ...                                         [#COVID19]\n",
              "3         499  ...  [#covid19, #cavalierkingcharles, #daftdug, #La...\n",
              "4         500  ...                           [#Covid_19, #medtwitter]\n",
              "...       ...  ...                                                ...\n",
              "11415  475039  ...      [#WHO, #COVID19, #coronavirus, #GlobalHealth]\n",
              "11416  475072  ...           [#COVID19, #everyproblemisanopportunity]\n",
              "11417  475075  ...                           [#coronavirus, #Science]\n",
              "11418  475191  ...                                         [#COVID19]\n",
              "11419  475199  ...                             [#CoronavirusPandemic]\n",
              "\n",
              "[230769 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfZnfDvC2YaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1.reset_index(inplace=True)\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "def clean_tweets(tweet):\n",
        " \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "    #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Äö√Ñ¬∂', '', tweet)\n",
        "#replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "#remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "#filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "#looping through conditions\n",
        "    for w in word_tokens:\n",
        "#check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDQ3WZHFKioz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re #regular expressions\n",
        "import string \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "reuse = False\n",
        "i =0 \n",
        "\n",
        "for tweet in d1[\"text\"]:\n",
        "  \n",
        "   d1.at[i,\"text\"] = clean_tweets(tweet)\n",
        "   i = i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhuFdj762YUR",
        "colab_type": "code",
        "outputId": "6a0fd934-dd54-4aab-f594-f0477fa3dc3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d1"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>293</td>\n",
              "      <td>ClayTravis FLU This seems fishy Covid_19 numbe...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:16Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:16Z</td>\n",
              "      <td>[#Covid_19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>393</td>\n",
              "      <td>netflix omg Ozark ending die Thanks distractio...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:29Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:29Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>481</td>\n",
              "      <td>I n't think gd leadership across states I see ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:39Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:39Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>499</td>\n",
              "      <td>In change conversation covid19 Charlie watchin...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#covid19, #cavalierkingcharles, #daftdug, #La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>500</td>\n",
              "      <td>Any hospital Our engineering department helped...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#Covid_19, #medtwitter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230764</th>\n",
              "      <td>11415</td>\n",
              "      <td>475039</td>\n",
              "      <td>‚Äú The WHO bureaucratic frustrating timid ‚Äî ind...</td>\n",
              "      <td>en</td>\n",
              "      <td>CA</td>\n",
              "      <td>2020-04-15T23:59:07Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:07Z</td>\n",
              "      <td>[#WHO, #COVID19, #coronavirus, #GlobalHealth]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230765</th>\n",
              "      <td>11416</td>\n",
              "      <td>475072</td>\n",
              "      <td>CEO_CleMetParks sir Some positive offshoots CO...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#COVID19, #everyproblemisanopportunity]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230766</th>\n",
              "      <td>11417</td>\n",
              "      <td>475075</td>\n",
              "      <td>The good news Iranian Revolutionary Guards bui...</td>\n",
              "      <td>en</td>\n",
              "      <td>IR</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#coronavirus, #Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230767</th>\n",
              "      <td>11418</td>\n",
              "      <td>475191</td>\n",
              "      <td>Remember Soviet Union told world advanced nucl...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:46Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:46Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230768</th>\n",
              "      <td>11419</td>\n",
              "      <td>475199</td>\n",
              "      <td>UMNExtFD UMNExt FOX9 And 's story video link T...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:47Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:47Z</td>\n",
              "      <td>[#CoronavirusPandemic]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230769 rows √ó 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        level_0  ...                                            hashtag\n",
              "0             0  ...                                        [#Covid_19]\n",
              "1             1  ...                                         [#COVID19]\n",
              "2             2  ...                                         [#COVID19]\n",
              "3             3  ...  [#covid19, #cavalierkingcharles, #daftdug, #La...\n",
              "4             4  ...                           [#Covid_19, #medtwitter]\n",
              "...         ...  ...                                                ...\n",
              "230764    11415  ...      [#WHO, #COVID19, #coronavirus, #GlobalHealth]\n",
              "230765    11416  ...           [#COVID19, #everyproblemisanopportunity]\n",
              "230766    11417  ...                           [#coronavirus, #Science]\n",
              "230767    11418  ...                                         [#COVID19]\n",
              "230768    11419  ...                             [#CoronavirusPandemic]\n",
              "\n",
              "[230769 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHWyEg9t2YRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1['text']= d1['text'].replace({'\\n':\" \",\"\\t\":\" \"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH_vKw0Jwmvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "import re \n",
        "import numpy as np \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M32Kho3GwmsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i =0 \n",
        "\n",
        "for tweet in d1[\"text\"]:\n",
        "  \n",
        "   d1.at[i,\"text\"] = str(tweet).lower()\n",
        "   \n",
        "   i = i + 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNK5syp4O_yJ",
        "colab_type": "code",
        "outputId": "552d5880-4332-414a-c219-a262f3cbe9d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d1"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>293</td>\n",
              "      <td>claytravis flu this seems fishy covid_19 numbe...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:16Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:16Z</td>\n",
              "      <td>[#Covid_19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>393</td>\n",
              "      <td>netflix omg ozark ending die thanks distractio...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:29Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:29Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>481</td>\n",
              "      <td>i n't think gd leadership across states i see ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:39Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:39Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>499</td>\n",
              "      <td>in change conversation covid19 charlie watchin...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#covid19, #cavalierkingcharles, #daftdug, #La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>500</td>\n",
              "      <td>any hospital our engineering department helped...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#Covid_19, #medtwitter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230764</th>\n",
              "      <td>11415</td>\n",
              "      <td>475039</td>\n",
              "      <td>‚Äú the who bureaucratic frustrating timid ‚Äî ind...</td>\n",
              "      <td>en</td>\n",
              "      <td>CA</td>\n",
              "      <td>2020-04-15T23:59:07Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:07Z</td>\n",
              "      <td>[#WHO, #COVID19, #coronavirus, #GlobalHealth]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230765</th>\n",
              "      <td>11416</td>\n",
              "      <td>475072</td>\n",
              "      <td>ceo_clemetparks sir some positive offshoots co...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#COVID19, #everyproblemisanopportunity]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230766</th>\n",
              "      <td>11417</td>\n",
              "      <td>475075</td>\n",
              "      <td>the good news iranian revolutionary guards bui...</td>\n",
              "      <td>en</td>\n",
              "      <td>IR</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#coronavirus, #Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230767</th>\n",
              "      <td>11418</td>\n",
              "      <td>475191</td>\n",
              "      <td>remember soviet union told world advanced nucl...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:46Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:46Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230768</th>\n",
              "      <td>11419</td>\n",
              "      <td>475199</td>\n",
              "      <td>umnextfd umnext fox9 and 's story video link t...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:47Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:47Z</td>\n",
              "      <td>[#CoronavirusPandemic]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230769 rows √ó 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        level_0  ...                                            hashtag\n",
              "0             0  ...                                        [#Covid_19]\n",
              "1             1  ...                                         [#COVID19]\n",
              "2             2  ...                                         [#COVID19]\n",
              "3             3  ...  [#covid19, #cavalierkingcharles, #daftdug, #La...\n",
              "4             4  ...                           [#Covid_19, #medtwitter]\n",
              "...         ...  ...                                                ...\n",
              "230764    11415  ...      [#WHO, #COVID19, #coronavirus, #GlobalHealth]\n",
              "230765    11416  ...           [#COVID19, #everyproblemisanopportunity]\n",
              "230766    11417  ...                           [#coronavirus, #Science]\n",
              "230767    11418  ...                                         [#COVID19]\n",
              "230768    11419  ...                             [#CoronavirusPandemic]\n",
              "\n",
              "[230769 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trNFedhFPFE7",
        "colab_type": "code",
        "outputId": "fe8351f1-6c79-418b-efeb-e586b4c86704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "d1"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level_0</th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>293</td>\n",
              "      <td>claytravis flu this seems fishy covid_19 numbe...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:16Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:16Z</td>\n",
              "      <td>[#Covid_19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>393</td>\n",
              "      <td>netflix omg ozark ending die thanks distractio...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:29Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:29Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>481</td>\n",
              "      <td>i n't think gd leadership across states i see ...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:39Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:39Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>499</td>\n",
              "      <td>in change conversation covid19 charlie watchin...</td>\n",
              "      <td>en</td>\n",
              "      <td>GB</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#covid19, #cavalierkingcharles, #daftdug, #La...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>500</td>\n",
              "      <td>any hospital our engineering department helped...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-03-29T00:00:42Z</td>\n",
              "      <td>2020-03-29</td>\n",
              "      <td>00:00:42Z</td>\n",
              "      <td>[#Covid_19, #medtwitter]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230764</th>\n",
              "      <td>11415</td>\n",
              "      <td>475039</td>\n",
              "      <td>‚Äú the who bureaucratic frustrating timid ‚Äî ind...</td>\n",
              "      <td>en</td>\n",
              "      <td>CA</td>\n",
              "      <td>2020-04-15T23:59:07Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:07Z</td>\n",
              "      <td>[#WHO, #COVID19, #coronavirus, #GlobalHealth]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230765</th>\n",
              "      <td>11416</td>\n",
              "      <td>475072</td>\n",
              "      <td>ceo_clemetparks sir some positive offshoots co...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#COVID19, #everyproblemisanopportunity]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230766</th>\n",
              "      <td>11417</td>\n",
              "      <td>475075</td>\n",
              "      <td>the good news iranian revolutionary guards bui...</td>\n",
              "      <td>en</td>\n",
              "      <td>IR</td>\n",
              "      <td>2020-04-15T23:59:15Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:15Z</td>\n",
              "      <td>[#coronavirus, #Science]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230767</th>\n",
              "      <td>11418</td>\n",
              "      <td>475191</td>\n",
              "      <td>remember soviet union told world advanced nucl...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:46Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:46Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230768</th>\n",
              "      <td>11419</td>\n",
              "      <td>475199</td>\n",
              "      <td>umnextfd umnext fox9 and 's story video link t...</td>\n",
              "      <td>en</td>\n",
              "      <td>US</td>\n",
              "      <td>2020-04-15T23:59:47Z</td>\n",
              "      <td>2020-04-15</td>\n",
              "      <td>23:59:47Z</td>\n",
              "      <td>[#CoronavirusPandemic]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230769 rows √ó 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        level_0  ...                                            hashtag\n",
              "0             0  ...                                        [#Covid_19]\n",
              "1             1  ...                                         [#COVID19]\n",
              "2             2  ...                                         [#COVID19]\n",
              "3             3  ...  [#covid19, #cavalierkingcharles, #daftdug, #La...\n",
              "4             4  ...                           [#Covid_19, #medtwitter]\n",
              "...         ...  ...                                                ...\n",
              "230764    11415  ...      [#WHO, #COVID19, #coronavirus, #GlobalHealth]\n",
              "230765    11416  ...           [#COVID19, #everyproblemisanopportunity]\n",
              "230766    11417  ...                           [#coronavirus, #Science]\n",
              "230767    11418  ...                                         [#COVID19]\n",
              "230768    11419  ...                             [#CoronavirusPandemic]\n",
              "\n",
              "[230769 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuAbO7AIgWty",
        "colab_type": "code",
        "outputId": "cb04aa50-11bc-47b7-c162-a2dfa14ed2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "import os, sys, glob\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 500)\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "!pip install mglearn\n",
        "import mglearn\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Collecting mglearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/38/8aced26fce0b2ae82c3c87cd3b6105f38ca6d9d51704ecc44aa54473e6b9/mglearn-0.1.9.tar.gz (540kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 542kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.18.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from mglearn) (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from mglearn) (1.0.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from mglearn) (7.0.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from mglearn) (2.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from mglearn) (0.15.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->mglearn) (2.4.7)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->mglearn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mglearn) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler->mglearn) (1.12.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582638 sha256=53a4cb5140ad22f597b454113394051893e4c1584c567cce461d020ca63324ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/a6/ea/a6a3716233fa62fc561259b5cb1e28f79e9ff3592c0adac5f0\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrPFYFm_rdc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NMF_model(keyword,data,max_df,min_df,n_components, n_grams):\n",
        "    tfidf = TfidfVectorizer(ngram_range=n_grams, max_df=max_df,min_df = min_df) \n",
        "    X = tfidf.fit_transform(data)\n",
        "\n",
        "    # Fit the model\n",
        "    nmf = NMF(n_components=n_components,random_state=0)\n",
        "    topics = nmf.fit_transform(X)\n",
        "\n",
        "    # Normalize\n",
        "    normalizer = Normalizer()\n",
        "    topics_norm = normalizer.fit_transform(topics)\n",
        "    \n",
        "    # Assigning component number to each document\n",
        "    topic_number = np.argmax(topics_norm,axis=1)\n",
        "    \n",
        "    # Counting number of documents in each component\n",
        "    counts = pd.Series(topic_number).value_counts()\n",
        "    \n",
        "    # Plotting number of documents in each component\n",
        "    plt.bar(pd.Series(topic_number).unique(),counts)\n",
        "    \n",
        "    # Top 10 words for each component\n",
        "    d = nmf.components_\n",
        "    w = tfidf.get_feature_names()\n",
        "    words = []\n",
        "    for r in range(len(d)):\n",
        "        a = sorted([(v,i) for i,v in enumerate(d[r])],reverse=True)[0:20]\n",
        "        words.append([w[e[1]] for e in a])\n",
        "    \n",
        "    # Printing words per topic\n",
        "    print('\\n Topics\\n')\n",
        "    print(keyword)\n",
        "    feature_names = np.array(tfidf.get_feature_names())\n",
        "    sorting = np.argsort(nmf.components_, axis=1)[:, ::-1]\n",
        "    mglearn.tools.print_topics(topics=range(n_components), feature_names = feature_names, sorting=sorting, topics_per_chunk=5,n_words=20)\n",
        "\n",
        "    return topics_norm, topic_number, words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0eJDMhyriR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "import scipy as sp;\n",
        "import sklearn;\n",
        "import sys;\n",
        "from nltk.corpus import stopwords;\n",
        "import nltk;\n",
        "from gensim.models import ldamodel\n",
        "import gensim.corpora;\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
        "from sklearn.decomposition import NMF;\n",
        "from sklearn.preprocessing import normalize;\n",
        "import pickle;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b2wJKd_riL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_text = d1[['text']];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EGv6CGRriIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d47c4e32-6d1a-447c-f959-0d680af97d10"
      },
      "source": [
        "data_text = data_text.astype('str');\n",
        "for idx in range(len(data_text)):\n",
        "    \n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['text'] = [word for word in data_text.iloc[idx]['text'].split(' ') if word not in stopwords.words()];\n",
        "    \n",
        "    #print logs to monitor output\n",
        "    if idx % 1000 == 0:\n",
        "        sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)));\n",
        "#save data because it takes very long to remove stop words\n",
        "pickle.dump(data_text, open('data_text.dat', 'wb'))\n",
        "#get the words as an array for lda input\n",
        "train_headlines = [value[0] for value in data_text.iloc[0:].values];"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c = 1000 / 230769"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-573f3d607313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#go through each word in each data_text row, remove stopwords, and set them on the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print logs to monitor output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-573f3d607313>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#go through each word in each data_text row, remove stopwords, and set them on the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print logs to monitor output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayOuYsGoriFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ0sXmWpriBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf27MdPtrh-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLAjYfqarh4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-xK9bknrh0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xIFk9-6rhxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}