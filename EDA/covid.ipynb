{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVmGmV-bATea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf2BooEHAxt1",
        "colab_type": "code",
        "outputId": "00df5351-a21a-4b93-ce8b-754a14ba933f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install  kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whzq_iGXExin",
        "colab_type": "code",
        "outputId": "37f0b4af-6a52-49e9-fb78-59f33a161340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir .kaggle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6OgN-FExF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"abolimarathe\",\"key\":\"53f6206052b7e46e5736781e4a76ca15\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7ry-eSAA1xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50MjTVKgCqf5",
        "colab_type": "code",
        "outputId": "49068c9f-4c93-41c7-cd63-8b38a2bf26c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle config set -n path -v{/content}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- path is now set to: {/content}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbeJXRP7DQVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jICkHxU-HHMa",
        "colab_type": "code",
        "outputId": "b428270e-6a5a-4676-9539-64dddf839e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                           title                                                size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  \n",
            "allen-institute-for-ai/CORD-19-research-challenge             COVID-19 Open Research Dataset Challenge (CORD-19)    3GB  2020-06-03 23:32:41          71785  \n",
            "roche-data-science-coalition/uncover                          UNCOVER COVID-19 Challenge                          179MB  2020-05-21 18:57:53          10483  \n",
            "kushshah95/the-insurance-company-tic-benchmark                The Insurance Company (TIC) Benchmark               262KB  2020-05-27 16:36:41              1  \n",
            "kianwee/agricultural-raw-material-prices-19902020             Agricultural Raw Material prices (1990-2020)         23KB  2020-05-27 04:51:29              0  \n",
            "jessicali9530/animal-crossing-new-horizons-nookplaza-dataset  Animal Crossing New Horizons NookPlaza Catalog      577KB  2020-05-18 22:50:26             42  \n",
            "rmjacobsen/property-listings-for-5-south-american-countries   Property Listings for 5 South American Countries    475MB  2020-05-25 02:05:38              1  \n",
            "biancaferreira/african-wildlife                               African Wildlife                                    448MB  2020-05-25 13:42:19              2  \n",
            "siddharthm1698/coursera-course-dataset                        Coursera Course Dataset                              23KB  2020-05-25 05:52:27             10  \n",
            "yamaerenay/spotify-dataset-19212020-160k-tracks               Spotify Dataset 1921-2020, 160k+ Tracks              17MB  2020-05-25 11:31:07             24  \n",
            "ruchi798/tv-shows-on-netflix-prime-video-hulu-and-disney      TV shows on Netflix, Prime Video, Hulu and Disney+   88KB  2020-05-25 15:38:39             12  \n",
            "ruchi798/movies-on-netflix-prime-video-hulu-and-disney        Movies on Netflix, Prime Video, Hulu and Disney+    627KB  2020-05-22 23:48:01             47  \n",
            "ruchi798/malnutrition-across-the-globe                        Malnutrition across the globe                        79KB  2020-05-25 09:51:45             14  \n",
            "stefanlarson/outofscope-intent-classification-dataset         Out-of-Scope Intent Classification Dataset          285KB  2020-05-15 03:04:34              1  \n",
            "gomes555/road-transport-brazil                                Road transport dataset in brazil                     62MB  2020-05-26 02:52:34              4  \n",
            "devinaconley/covid19-mobility-data                            COVID-19 Google mobility data                         4MB  2020-06-04 12:15:15              2  \n",
            "benroshan/factors-affecting-campus-placement                  Campus Recruitment                                    5KB  2020-04-11 11:09:02           5523  \n",
            "bobbyscience/league-of-legends-diamond-ranked-games-10-min    League of Legends Diamond Ranked Games (10 min)     539KB  2020-04-13 13:53:02           2542  \n",
            "fireballbyedimyrnmom/us-counties-covid-19-dataset             US counties COVID 19 dataset                          2MB  2020-06-04 11:17:03           7247  \n",
            "divyansh22/flight-delay-prediction                            January Flight Delay Prediction                      23MB  2020-04-14 13:15:41           1850  \n",
            "clmentbisaillon/fake-and-real-news-dataset                    Fake and real news dataset                           41MB  2020-03-26 18:51:15           4998  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd8t6jfjHHJk",
        "colab_type": "code",
        "outputId": "fb03dec2-3d0c-40dd-8a58-fd19fcad63cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!kaggle datasets list -s early\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "ref                                                     title                                            size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------  ----------------------------------------------  -----  -------------------  -------------  \n",
            "agatii/total-sale-2018-yearly-data-of-grocery-shop      TOTAL SALE 2018 Yearly data of grocery shop.      1MB  2019-07-07 19:09:47           3284  \n",
            "cwiloc/climate-data-from-ocean-ships                    Ocean Ship Logbooks (1750-1850)                  19MB  2017-11-16 00:59:48           3375  \n",
            "smid80/coronavirus-covid19-tweets-early-april           Coronavirus (covid19) Tweets - early April        1GB  2020-04-30 14:30:50            686  \n",
            "aaronschlegel/seattle-pet-licenses                      Seattle Pet Licenses                            924KB  2018-05-15 01:26:45           2543  \n",
            "kemical/kickstarter-projects                            Kickstarter Projects                             37MB  2018-02-08 09:02:30          37830  \n",
            "therohk/million-headlines                               A Million News Headlines                         20MB  2020-04-27 13:07:30          19353  \n",
            "rtatman/six-degrees-of-francis-bacon                    Six Degrees of Francis Bacon                      3MB  2017-08-29 21:19:29            618  \n",
            "ruslankl/early-biomarkers-of-parkinsons-disease         Early Biomarkers of Parkinson's Disease          65KB  2019-01-06 08:48:47            611  \n",
            "smid80/coronavirus-covid19-tweets                       Coronavirus (covid19) Tweets                      2KB  2020-05-12 04:24:57           3764  \n",
            "lsind18/weight-vs-age-of-chicks-on-different-diets      Weight vs Age of chicks on different diets        3KB  2019-10-22 07:32:58            336  \n",
            "crowdflower/first-gop-debate-twitter-sentiment          First GOP Debate Twitter Sentiment                2MB  2019-11-17 21:18:37          16756  \n",
            "sudalairajkumar/covid19-in-italy                        COVID-19 in Italy                               145KB  2020-06-04 10:13:37           6366  \n",
            "chicago/chicago-early-learning-programs                 Chicago Early Learning Programs                 106KB  2019-12-05 23:19:11             99  \n",
            "taindow/earlystoppingpytorch                            early-stopping-pytorch                            2MB  2019-07-20 08:17:48            139  \n",
            "groundhogclub/groundhog-day                             Groundhog Day Forecasts and Temperatures          3KB  2017-01-31 20:35:23           1172  \n",
            "unanimad/corona-virus-brazil                            Coronavirus - Brazil                              2MB  2020-06-04 11:56:31          15201  \n",
            "valkling/tappy-keystroke-data-with-parkinsons-patients  Tappy Keystroke Data with Parkinson's Patients   96MB  2018-02-04 05:41:47            908  \n",
            "kveykva/sf-bay-area-pokemon-go-spawns                   SF Bay Area Pokemon Go Spawns                     4MB  2016-09-15 04:12:39           1127  \n",
            "loveall/cervical-cancer-risk-classification             Cervical Cancer Risk Classification               9KB  2017-08-31 01:02:22          12897  \n",
            "monsieurwagner/nyctransit                               NYC Transit Data                                147MB  2017-07-22 00:52:54            658  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEJJgFbCHHGi",
        "colab_type": "code",
        "outputId": "27776b1f-7844-4a1f-b775-6aede0838f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!kaggle datasets download -d smid80/coronavirus-covid19-tweets-early-april -p /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coronavirus-covid19-tweets-early-april.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IASZVTWcK_Fq",
        "colab_type": "code",
        "outputId": "a0475f81-bede-48f4-ad42-8768861a471d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip \\*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  coronavirus-covid19-tweets-early-april.zip\n",
            "replace 2020-03-29 Coronavirus Tweets.CSV? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ACJ8PhYU8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7kHcEVcHG9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1 = pd.read_csv(\"2020-03-29 Coronavirus Tweets.CSV\")\n",
        "d2 = pd.read_csv(\"2020-03-30 Coronavirus Tweets.CSV\")\n",
        "d3 = pd.read_csv(\"2020-03-31 Coronavirus Tweets.CSV\")\n",
        "d4 = pd.read_csv(\"2020-04-01 Coronavirus Tweets.CSV\")\n",
        "d5 = pd.read_csv(\"2020-04-02 Coronavirus Tweets.CSV\")\n",
        "d6 = pd.read_csv(\"2020-04-03 Coronavirus Tweets.CSV\")\n",
        "d7 = pd.read_csv(\"2020-04-04 Coronavirus Tweets.CSV\")\n",
        "d8 = pd.read_csv(\"2020-04-05 Coronavirus Tweets.CSV\")\n",
        "d9 = pd.read_csv(\"2020-04-06 Coronavirus Tweets.CSV\")\n",
        "d10 = pd.read_csv(\"2020-04-07 Coronavirus Tweets.CSV\")\n",
        "d11 = pd.read_csv(\"2020-04-08 Coronavirus Tweets.CSV\")\n",
        "d12 = pd.read_csv(\"2020-04-09 Coronavirus Tweets.CSV\")\n",
        "d13 = pd.read_csv(\"2020-04-10 Coronavirus Tweets.CSV\")\n",
        "d14 = pd.read_csv(\"2020-04-11 Coronavirus Tweets.CSV\")\n",
        "d15 = pd.read_csv(\"2020-04-12 Coronavirus Tweets.CSV\")\n",
        "d16 = pd.read_csv(\"2020-04-13 Coronavirus Tweets.CSV\")\n",
        "d17 = pd.read_csv(\"2020-04-14 Coronavirus Tweets.CSV\")\n",
        "d18 = pd.read_csv(\"2020-04-15 Coronavirus Tweets.CSV\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFu5kXbQHGxA",
        "colab_type": "code",
        "outputId": "6d65a0c1-4920-4bf9-f440-3ec26b7735e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "from gensim import models, corpora \n",
        "import spacy\n",
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.tokenizer import _get_regex_pattern\n",
        "nlp = spacy.load(\"en_core_web_sm\") \n",
        "import numpy as np \n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew-a9LRvLJy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "d1= d1[['text','lang','country_code','created_at']]\n",
        "d1 = d1[d1['lang']=='en']\n",
        "\n",
        "d2= d2[['text','lang','country_code','created_at']]\n",
        "d2 = d2[d2['lang']=='en']\n",
        "\n",
        "d3= d3[['text','lang','country_code','created_at']]\n",
        "d3 = d3[d3['lang']=='en']\n",
        "\n",
        "d4= d4[['text','lang','country_code','created_at']]\n",
        "d4 = d4[d4['lang']=='en']\n",
        "\n",
        "d5= d5[['text','lang','country_code','created_at']]\n",
        "d5 = d5[d5['lang']=='en']\n",
        "\n",
        "d6= d6[['text','lang','country_code','created_at']]\n",
        "d6 = d6[d6['lang']=='en']\n",
        "\n",
        "d7= d7[['text','lang','country_code','created_at']]\n",
        "d7 = d7[d7['lang']=='en']\n",
        "\n",
        "d8= d8[['text','lang','country_code','created_at']]\n",
        "d8 = d8[d8['lang']=='en']\n",
        "\n",
        "d9= d9[['text','lang','country_code','created_at']]\n",
        "d9 = d9[d9['lang']=='en']\n",
        "\n",
        "d10= d10[['text','lang','country_code','created_at']]\n",
        "d10 = d10[d10['lang']=='en']\n",
        "\n",
        "d11= d11[['text','lang','country_code','created_at']]\n",
        "d11 = d11[d11['lang']=='en']\n",
        "\n",
        "d12= d12[['text','lang','country_code','created_at']]\n",
        "d12 = d12[d12['lang']=='en']\n",
        "\n",
        "d13= d13[['text','lang','country_code','created_at']]\n",
        "d13 = d13[d13['lang']=='en']\n",
        "\n",
        "d14= d14[['text','lang','country_code','created_at']]\n",
        "d14 = d14[d14['lang']=='en']\n",
        "\n",
        "d15= d15[['text','lang','country_code','created_at']]\n",
        "d15 = d15[d15['lang']=='en']\n",
        "\n",
        "d16= d16[['text','lang','country_code','created_at']]\n",
        "d16 = d16[d16['lang']=='en']\n",
        "\n",
        "d17= d17[['text','lang','country_code','created_at']]\n",
        "d17 = d17[d17['lang']=='en']\n",
        "\n",
        "d18= d18[['text','lang','country_code','created_at']]\n",
        "d18 = d18[d18['lang']=='en']\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNPLWAZrXvP_",
        "colab_type": "code",
        "outputId": "54622c43-674b-460a-f734-9e30dfc98ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install tweet-preprocessor\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFhtAfkzXvMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re #regular expressions\n",
        "import string \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import preprocessor as p\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "reuse = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67grH7WKXvIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "\n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "#Emoji patterns\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "         u\"\\U00002702-\\U000027B0\"\n",
        "         u\"\\U000024C2-\\U0001F251\"\n",
        "         \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "#combine sad and happy emoticons\n",
        "emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "def clean_tweets(tweet):\n",
        " \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(tweet)\n",
        "    #removing mentions\n",
        "    tweet = re.sub(r':', '', tweet)\n",
        "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "#replace consecutive non-ASCII characters with a space\n",
        "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "#remove emojis from tweet\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "#filter using NLTK library append it to a string\n",
        "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "    filtered_tweet = []\n",
        "#looping through conditions\n",
        "    for w in word_tokens:\n",
        "#check tokens against stop words , emoticons and punctuations\n",
        "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "            filtered_tweet.append(w)\n",
        "    return ' '.join(filtered_tweet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzyJeCZpXu6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d1['text']= d1['text'].replace({'\\n':\" \",\"\\t\":\" \"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3QzOtq6Ya8o",
        "colab_type": "code",
        "outputId": "db22defd-1b92-40c7-d006-c4025d56af50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "hashtags = d1['text'].apply(lambda x: re.findall(r'\\B#\\w*[a-zA-Z]+\\w*',x))\n",
        "print(hashtags)\n",
        "print(len(hashtags))\n",
        "d1['hashtag'] = hashtags"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2               [#minneapolis, #mn, #covid19, #coronavirus]\n",
            "6                                     [#COVID19, #pandemic]\n",
            "7                  [#Wuhan, #COVID19, #CoronavirusOutbreak]\n",
            "9                           [#Coronavirus, #covid19, #lgbt]\n",
            "14                                [#CoronaUpdate, #Covid19]\n",
            "                                ...                        \n",
            "564135                          [#Trump, #coronavirus, #US]\n",
            "564136                                [#NYPD, #coronavirus]\n",
            "564137                                           [#COVID19]\n",
            "564138                             [#coronavirus, #COVID19]\n",
            "564140    [#TrumpVirus, #coronavirus, #Covid_19, #EveryL...\n",
            "Name: text, Length: 313036, dtype: object\n",
            "313036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmZp0f3CYd8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.dataframe as dsk\n",
        "import multiprocessing as mp\n",
        "from dask.multiprocessing import get\n",
        "\n",
        "ddf = dsk.from_pandas(d1,npartitions=4*mp.cpu_count())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hMg8RaNYd5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddf['text'] = ddf['text'].map_partitions(lambda x: x.apply(p.clean),meta=('clean_text','str')).compute(scheduler='processes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brARmjkIYd1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return [blob.sentiment.polarity]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E2Rttw-Ydxo",
        "colab_type": "code",
        "outputId": "6b6e8925-98fc-49b1-d805-79f1b13909b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df = ddf.compute(scheduler='processes')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>People are just storing up. They are staying a...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#minneapolis, #mn, #covid19, #coronavirus]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>. spoke with about the stimulus package making...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#COVID19, #pandemic]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>First medical team aiding in fight against epi...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Wuhan, #COVID19, #CoronavirusOutbreak]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>.: Is 'Lying' About Testing</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Coronavirus, #covid19, #lgbt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>| Johns Hopkins University has said it did not...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#CoronaUpdate, #Covid19]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ...                                      hashtag\n",
              "2   People are just storing up. They are staying a...  ...  [#minneapolis, #mn, #covid19, #coronavirus]\n",
              "6   . spoke with about the stimulus package making...  ...                        [#COVID19, #pandemic]\n",
              "7   First medical team aiding in fight against epi...  ...     [#Wuhan, #COVID19, #CoronavirusOutbreak]\n",
              "9                         .: Is 'Lying' About Testing  ...              [#Coronavirus, #covid19, #lgbt]\n",
              "14  | Johns Hopkins University has said it did not...  ...                    [#CoronaUpdate, #Covid19]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37qiQw5uYdtz",
        "colab_type": "code",
        "outputId": "669c9586-29e5-4419-911f-ca323bdc2948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return [blob.sentiment.polarity, blob.sentiment.subjectivity]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df.reset_index(inplace = True)\n",
        "polarity,subjectivity = [], []\n",
        "for i in range(len(df)):\n",
        "    pol , sub = get_sentiment(df['text'][i])\n",
        "    polarity.append(pol)\n",
        "    subjectivity.append(sub)\n",
        "    \n",
        "df['polarity'] = polarity\n",
        "df['subjectivity'] = subjectivity\n",
        "df.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>People are just storing up. They are staying a...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#minneapolis, #mn, #covid19, #coronavirus]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>. spoke with about the stimulus package making...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#COVID19, #pandemic]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>First medical team aiding in fight against epi...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Wuhan, #COVID19, #CoronavirusOutbreak]</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>.: Is 'Lying' About Testing</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Coronavirus, #covid19, #lgbt]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>| Johns Hopkins University has said it did not...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#CoronaUpdate, #Covid19]</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ... subjectivity\n",
              "0      2  ...     0.000000\n",
              "1      6  ...     0.000000\n",
              "2      7  ...     0.166667\n",
              "3      9  ...     0.000000\n",
              "4     14  ...     0.000000\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVquAm0QYdps",
        "colab_type": "code",
        "outputId": "c2db61f7-1e63-4eb2-82da-d0bb0fdac0ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")   \n",
        "import numpy as np \n",
        "from nltk.corpus import stopwords\n",
        "import nltk \n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COuJMhJweMPT",
        "colab_type": "code",
        "outputId": "c88db204-5cb7-42a2-a814-5cd4ede687d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIJ4LrUyeMHY",
        "colab_type": "code",
        "outputId": "0e52f9f1-253a-4a1c-b656-78bb0aed1050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "nltk.word_tokenize(df[\"text\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['People',\n",
              " 'are',\n",
              " 'just',\n",
              " 'storing',\n",
              " 'up',\n",
              " '.',\n",
              " 'They',\n",
              " 'are',\n",
              " 'staying',\n",
              " 'at',\n",
              " 'home',\n",
              " 'freezing',\n",
              " 'things',\n",
              " '.',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'to',\n",
              " 'dealers',\n",
              " 'around',\n",
              " 'the',\n",
              " 'state',\n",
              " 'and',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'area',\n",
              " ',',\n",
              " 'they',\n",
              " 'are',\n",
              " 'all',\n",
              " 'out',\n",
              " 'of',\n",
              " 'freezers',\n",
              " ',',\n",
              " 'said',\n",
              " 'owner',\n",
              " 'Frank',\n",
              " 'Mertz',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajOSJwdIeMDR",
        "colab_type": "code",
        "outputId": "71c4ee36-8c44-4d6f-adac-59da1638ab46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "nltk.tokenize.WhitespaceTokenizer().tokenize(df[\"text\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['People',\n",
              " 'are',\n",
              " 'just',\n",
              " 'storing',\n",
              " 'up.',\n",
              " 'They',\n",
              " 'are',\n",
              " 'staying',\n",
              " 'at',\n",
              " 'home',\n",
              " 'freezing',\n",
              " 'things.',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'to',\n",
              " 'dealers',\n",
              " 'around',\n",
              " 'the',\n",
              " 'state',\n",
              " 'and',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'area,',\n",
              " 'they',\n",
              " 'are',\n",
              " 'all',\n",
              " 'out',\n",
              " 'of',\n",
              " 'freezers,',\n",
              " 'said',\n",
              " 'owner',\n",
              " 'Frank',\n",
              " 'Mertz.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brYOEEGVeL8S",
        "colab_type": "code",
        "outputId": "9c68050b-7f44-4a63-d8a3-b5d0c2e3193e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "nltk.tokenize.WordPunctTokenizer().tokenize(df[\"text\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['People',\n",
              " 'are',\n",
              " 'just',\n",
              " 'storing',\n",
              " 'up',\n",
              " '.',\n",
              " 'They',\n",
              " 'are',\n",
              " 'staying',\n",
              " 'at',\n",
              " 'home',\n",
              " 'freezing',\n",
              " 'things',\n",
              " '.',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'to',\n",
              " 'dealers',\n",
              " 'around',\n",
              " 'the',\n",
              " 'state',\n",
              " 'and',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'area',\n",
              " ',',\n",
              " 'they',\n",
              " 'are',\n",
              " 'all',\n",
              " 'out',\n",
              " 'of',\n",
              " 'freezers',\n",
              " ',',\n",
              " 'said',\n",
              " 'owner',\n",
              " 'Frank',\n",
              " 'Mertz',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISv6MeoReL4s",
        "colab_type": "code",
        "outputId": "30c444f6-6e43-4bce-f52a-142ed2138927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "nltk.tokenize.TreebankWordTokenizer().tokenize(df[\"text\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['People',\n",
              " 'are',\n",
              " 'just',\n",
              " 'storing',\n",
              " 'up.',\n",
              " 'They',\n",
              " 'are',\n",
              " 'staying',\n",
              " 'at',\n",
              " 'home',\n",
              " 'freezing',\n",
              " 'things.',\n",
              " 'I',\n",
              " 'talk',\n",
              " 'to',\n",
              " 'dealers',\n",
              " 'around',\n",
              " 'the',\n",
              " 'state',\n",
              " 'and',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'area',\n",
              " ',',\n",
              " 'they',\n",
              " 'are',\n",
              " 'all',\n",
              " 'out',\n",
              " 'of',\n",
              " 'freezers',\n",
              " ',',\n",
              " 'said',\n",
              " 'owner',\n",
              " 'Frank',\n",
              " 'Mertz',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLieGDs5eLyB",
        "colab_type": "code",
        "outputId": "73e50352-212f-4c67-8721-22c17b455aaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(df[\"text\"][0])\n",
        "df1 = pd.DataFrame()\n",
        "df1['OriginalWords'] = pd.Series(words)\n",
        "#porter's stemmer\n",
        "porterStemmedWords = [nltk.stem.PorterStemmer().stem(word) for word in words]\n",
        "df1['PorterStemmedWords'] = pd.Series(porterStemmedWords)\n",
        "#SnowBall stemmer\n",
        "snowballStemmedWords = [nltk.stem.SnowballStemmer(\"english\").stem(word) for word in words]\n",
        "da = pd.DataFrame()\n",
        "da['SnowballStemmedWords'] = pd.Series(snowballStemmedWords)\n",
        "da"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SnowballStemmedWords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>peopl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>just</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>store</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>they</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>stay</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>freez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>things.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>talk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dealer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>area,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>they</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>freezers,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>owner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>frank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>mertz.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SnowballStemmedWords\n",
              "0                 peopl\n",
              "1                   are\n",
              "2                  just\n",
              "3                 store\n",
              "4                   up.\n",
              "5                  they\n",
              "6                   are\n",
              "7                  stay\n",
              "8                    at\n",
              "9                  home\n",
              "10                freez\n",
              "11              things.\n",
              "12                    i\n",
              "13                 talk\n",
              "14                   to\n",
              "15               dealer\n",
              "16               around\n",
              "17                  the\n",
              "18                state\n",
              "19                  and\n",
              "20                  out\n",
              "21                   of\n",
              "22                  the\n",
              "23                area,\n",
              "24                 they\n",
              "25                  are\n",
              "26                  all\n",
              "27                  out\n",
              "28                   of\n",
              "29            freezers,\n",
              "30                 said\n",
              "31                owner\n",
              "32                frank\n",
              "33               mertz."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueK_ezJNeLu2",
        "colab_type": "code",
        "outputId": "3a823690-a4ee-4dfd-9965-f9c7057a0c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "words  = nltk.tokenize.WhitespaceTokenizer().tokenize(df[\"text\"][0])\n",
        "df1 = pd.DataFrame()\n",
        "df1['OriginalWords'] = pd.Series(words)\n",
        "#WordNet Lemmatization\n",
        "\n",
        "da = pd.DataFrame()\n",
        "wordNetLemmatizedWords = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "da['WordNetLemmatizer'] = pd.Series(wordNetLemmatizedWords)\n",
        "\n",
        "da"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WordNetLemmatizer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>People</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>just</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>storing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>up.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>They</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>staying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>home</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>freezing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>things.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>talk</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>dealer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>area,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>they</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>are</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>freezers,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>owner</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Frank</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Mertz.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   WordNetLemmatizer\n",
              "0             People\n",
              "1                are\n",
              "2               just\n",
              "3            storing\n",
              "4                up.\n",
              "5               They\n",
              "6                are\n",
              "7            staying\n",
              "8                 at\n",
              "9               home\n",
              "10          freezing\n",
              "11           things.\n",
              "12                 I\n",
              "13              talk\n",
              "14                to\n",
              "15            dealer\n",
              "16            around\n",
              "17               the\n",
              "18             state\n",
              "19               and\n",
              "20               out\n",
              "21                of\n",
              "22               the\n",
              "23             area,\n",
              "24              they\n",
              "25               are\n",
              "26               all\n",
              "27               out\n",
              "28                of\n",
              "29         freezers,\n",
              "30              said\n",
              "31             owner\n",
              "32             Frank\n",
              "33            Mertz."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeKWtvhaeLsN",
        "colab_type": "code",
        "outputId": "5ccfdc0c-92a4-4add-9f7a-887aa3ab097d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>lang</th>\n",
              "      <th>country_code</th>\n",
              "      <th>created_at</th>\n",
              "      <th>hashtag</th>\n",
              "      <th>polarity</th>\n",
              "      <th>subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>People are just storing up. They are staying a...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#minneapolis, #mn, #covid19, #coronavirus]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>. spoke with about the stimulus package making...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#COVID19, #pandemic]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>First medical team aiding in fight against epi...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Wuhan, #COVID19, #CoronavirusOutbreak]</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>.: Is 'Lying' About Testing</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#Coronavirus, #covid19, #lgbt]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>| Johns Hopkins University has said it did not...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T00:00:00Z</td>\n",
              "      <td>[#CoronaUpdate, #Covid19]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313031</th>\n",
              "      <td>564135</td>\n",
              "      <td>extends guidelines, braces for big death toll.</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T23:59:58Z</td>\n",
              "      <td>[#Trump, #coronavirus, #US]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313032</th>\n",
              "      <td>564136</td>\n",
              "      <td>A rd member dies of after hundreds of officers...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T23:59:58Z</td>\n",
              "      <td>[#NYPD, #coronavirus]</td>\n",
              "      <td>0.227273</td>\n",
              "      <td>0.545455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313033</th>\n",
              "      <td>564137</td>\n",
              "      <td>For many students in our state, closures are i...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T23:59:59Z</td>\n",
              "      <td>[#COVID19]</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.283333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313034</th>\n",
              "      <td>564138</td>\n",
              "      <td>Will the pandemic of come to be seen as a mome...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T23:59:59Z</td>\n",
              "      <td>[#coronavirus, #COVID19]</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313035</th>\n",
              "      <td>564140</td>\n",
              "      <td>Is there a way to revoke someones Twitter acco...</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2020-03-29T23:59:59Z</td>\n",
              "      <td>[#TrumpVirus, #coronavirus, #Covid_19, #EveryL...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>313036 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         index  ... subjectivity\n",
              "0            2  ...     0.000000\n",
              "1            6  ...     0.000000\n",
              "2            7  ...     0.166667\n",
              "3            9  ...     0.000000\n",
              "4           14  ...     0.000000\n",
              "...        ...  ...          ...\n",
              "313031  564135  ...     0.100000\n",
              "313032  564136  ...     0.545455\n",
              "313033  564137  ...     0.283333\n",
              "313034  564138  ...     0.950000\n",
              "313035  564140  ...     0.000000\n",
              "\n",
              "[313036 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dUF6PIueLoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7G1AfQ6eLlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLJEXlWEYdiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIBwXQdlYdd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_KrLcfVYdV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPWcqVkQYdSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TIOhPSqYdOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}